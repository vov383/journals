---
title: 파이썬 머신러닝 완벽 가이드 섹션 3 정리
created: 2025-10-27 09:38
alias:
tags:
---

# 섹션 3
### 사이킷런 소개
쉽다
복잡한 머신러닝을 쉽게 구현 가능. 
api도 쉽고 직관적

다른 머신러닝 패키지들에 영향

많은 다양한 알고리즘을 제공

오랜 시간 실전환경에서 검증.
성숙한 라이브러리

Numpy, Scipy 기반 위에서 구축됨


#### 붓꽃 데이터 분류 실습
![[../../attachment/Pasted image 20251027094545.png]]

머신러닝의 헬로월드

예측모델
붓꽃 데이터 피처 4개

Sepal length
Sepal width
Petal length
Petal width

#### 머신러닝 용어정리
##### 피처 vs 속성
피처 == 속성
피처는 데이터 세트의 일반 속성

타겟값, 결정값을 학습

2차원 데이터의 컬럼속성.
머신러닝은 2차원 이상의 데이터를 사용
타겟값을 제외한 나머지 속성을 모두 피처로 지정

##### 레이블, 클래스, 타겟(값), 결정(값)
레이블 클래스 타겟(값) 결정(값)

타겟값 == 결정값
피처에 기반해서 학습된 값

타겟값이 분류일 경우에는 레이블 또는 클래스


#### 지도학습 - 분류
학습을 위한 피처들이 있고 그에 대한 답이 있음
피처들의 패턴을 기반으로 값을 학습

데이터의 패턴을 기반으로 ml 모델을 만들어.


![[../../attachment/Pasted image 20251027095601.png]]


#### 붓꽃 데이터 분류 예측 프로세스
![[../../attachment/Pasted image 20251027095539.png]]


![[../../attachment/Pasted image 20251027100659.png]]


#### 사이킷런 기반 프레임워크 
##### Estimator
모든 분류 구현 클래스의 최상위 부모 클래스

###### fit() 
학습
###### predict()
예측

##### 분류
###### Classifier

##### 회귀
###### Regressor


![[../../attachment/Pasted image 20251027100712.png]]


#### 사이킷런의 주요 모듈

![[../../attachment/Pasted image 20251027100923.png]]


#### 사이킷런 내장 예제 데이터 셋 - 분류 및 회귀용
![[../../attachment/Pasted image 20251027101107.png]]

#### 내장 예제 데이터 셋 구성
![[../../attachment/Pasted image 20251027101134.png]]


### 학습과 테스트 데이터 세트의 분리
![[../../attachment/Pasted image 20251027110611.png]]


![[../../attachment/Pasted image 20251027110700.png]]

피처와 타겟값

test_size=0.3
데이터의 30프로를 테스트 데이터 세트로 만들어주세요

random_state=121
random_state는 호출할 때마다 동일한 랜덤을 호출하기 위한 일종의 난수값


### 교차검증 K-Fold와 Stratified K-Fold의 이해 01
#### 교차검증
![[../../attachment/Pasted image 20251027111502.png]]

like 모의고사 : 수능

#### K-Fold 교차검증
![[../../attachment/Pasted image 20251027131738.png]]

몇 번 폴드하냐
학습 데이터 셋 5개 만들어.
4/5 학습, 검증 1/5

학습 & 검증을 5번 반복

10 폴드면 10번 학습, 검증은 1/10

![[../../attachment/Pasted image 20251027131922.png]]

불균형한 분포도를 가지는 레이블 데이터.
2만건 신용카드 데이터에서 사기건은 100건.
이러면 레이블이 0.5%밖에 안 된다.

불균형한 분포도를 갖고 있기 때문에 
검증 데이터 세트가 가지는 레이블 분포도와 유사하도록 검증 데이터 추출


### 교차검증 성능평가 
#### cross_val_score()

![[../../attachment/Pasted image 20251027134030.png]]

KFold 클래스를 쓰면 코드가 길어.
이걸 좀 더 간단하게 사용하기


cross_val_score(estimator, X, y=None, scoring=None, cv=None)


#### GridSearchCV
![[../../attachment/Pasted image 20251027134309.png]]
하이퍼 파리미터?
Classifier나 Regressor같은 알고리즘에 사용되는 하이퍼 파라미터

순차적으로 입력하면서 최적의 파라미터 도출 방안 제공

알고리즘 구성하는 뼈대 파라미터

grid는 격자
max_depth
min_samples_split

이걸 촘촘하게 설정해서 최적 하이퍼 파라미터 찾아준다.


### 데이터 전처리
#### 인코딩
![[../../attachment/Pasted image 20251027140211.png]]
머신러닝 모델 만들 때 굉장히 중요
쓰레기 넣으면 결과도 안 좋아.

문자열(x) -> 전부 숫자로 바꿔야.

데이터 스케일링 -> 척도를 바꾼다.
집 데이터. 방 3개, 집에서 역까지 1.5km

이상치 제거

피처 엔지니어링

#### 데이터 인코딩
![[../../attachment/Pasted image 20251027140443.png]]

###### 레이블 인코딩
![[../../attachment/Pasted image 20251027140450.png]]

문자열을 숫자형 값으로 변환
상품분류를 레이블 인코딩

```
items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']

인코딩 변환값: [0 1 4 5 3 3 2 2]
```

변환하면서 의도하지 않은 의미부여가 됨.
TV보다 냉장고의 인코딩 변환값이 크다.
이걸 하기 싫어서 사용하는 것이 

##### 원 핫 인코딩
![[../../attachment/Pasted image 20251027140654.png]]

하나만 핫해
하나만 1.

6개의 고유값. 벡터화.
```
원-핫 인코딩 데이터
[[1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0.]]
 
 원-핫 인코딩 데이터 차원
(8, 6)
```

사이킷런에서 이걸 어떻게?

![[../../attachment/Pasted image 20251027140808.png]]

약간 번거로워서 
pd.get_dummies(DataFrame)을 이용한다.


#### 스케일링
#### 피처 스케일링
![[../../attachment/Pasted image 20251027142535.png]]

표준화와 정규화
둘을 섞어서 쓰지만 엄밀히 말하면 구분됨.

표준화 : standization
0을 중심으로 

정규화 : normalization
0~1 값 범위로
-1~1 값 사이 범위로 변환도 할 수 있다.
그림 예시느 0~1 값 범위로 만든 것



![[../../attachment/Pasted image 20251027143254.png]]

대표적인 2가지
###### StandardScaler
평균 0, 분산 1. 정규분포 형태로 변환
###### MinMaxScaler
데이터를 0과 1 사이 범위 값으로
(음수 값 있으면 -1 ~ 1)

가급적이면 선형 계열 
피처들을 전부 스케일링 해준다.

LR, SVM

스케일링 해주면 좋아지는 경우가 많아서

분류의 Tree의 경우
스케일링에 큰 영향 X라서 안 한다.


### 타이타닉 생존자 예측모델

![[../../attachment/Pasted image 20251027145442.png]]

캐글 머신러닝 입문용 

로지스틱 회귀는 이름만 회귀이고 분류다.
