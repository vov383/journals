---
title: 비지도학습의 구체적인 예시로 오토인코더
created: 2025-09-16 08:39
alias:
tags:
---
오토인코더는 '정답' 없이 데이터 그 자체의 특징을 학습하는 비지도학습의 원리를 아주 잘 보여주는 모델

오토인코더의 핵심 아이디어
'**정보를 효율적으로 압축했다가 다시 펼치는 과정**'에서 데이터의 중요한 특징을 스스로 배우게 하는 것.

긴 글을 읽고 **핵심만 요약**한 뒤, 그 요약본만 보고 **원래 글로 복원**하려는 과정과 비슷
핵심(패턴, 특징)을 정말 잘 파악해야 가능하다.




오토인코더의 작동 방식
'요약하고 복원하는' 구조 3단계

- 1단계: 인코더 (Encoder)
    
    - 입력된 원본 데이터를 더 작은 차원의 정보로 '압축'하고 '요약'하는 부분
        
- 2단계: 잠재 공간 (Latent Space / Bottleneck)
    
    - 압축된 '핵심 요약본'이 저장되는 곳
    - 데이터의 가장 중요한 특징들이 응축되어 있음.
        
- 3단계: 디코더 (Decoder) 
    
    - 잠재 공간의 '핵심 요약본'을 보고 다시 원본 데이터 형태로 '복원'하고 '펼치는' 부분
        
알겠습니다. 인코더가 데이터를 핵심만 남기고 압축하는 '요약' 단계였다면, 그 다음은 바로 그 '핵심 요약본'이 저장되는 장소입니다.

### 2단계: 잠재 공간 (Latent Space / Bottleneck)

이곳은 **인코더가 만들어낸 압축된 데이터, 즉 데이터의 가장 중요한 정수(essence)가 머무는 공간**
'병목(Bottleneck)'이라는 별명처럼, 넓은 입구(원본 데이터)에서 들어온 정보가 아주 좁은 통로(잠재 공간)를 거쳐가는 모습

이 '병목'이 오토인코더 학습의 핵심 포인트입니다.

- **정보의 강제적 요약:** 일부러 공간을 좁게 만들어서, AI가 어쩔 수 없이 데이터의 **가장 중요한 특징(feature)만 남기고 불필요한 정보(noise)는 버리도록** 강제하는 것.
    
- **효율적인 표현 학습:** 이 과정을 통과했다는 것 자체가, 데이터의 핵심을 아주 효율적으로 표현하는 방법을 스스로 터득했다는 의미
    

자, 이렇게 데이터의 알짜배기만 남은 '핵심 요약본'이 준비되었습니다. 이제 마지막 단계는 이 요약본을 보고 원래의 모습으로 되돌리는 일이겠죠?

이 '복원' 작업을 수행하는 **디코더(Decoder)**는 어떻게 작동할지 함께 알아볼까요?

좋습니다. 데이터의 핵심만 담은 '요약본'이 잠재 공간에 준비되었으니, 이제 마지막 단계로 넘어가죠.

### **3단계: 디코더 (Decoder) 🖼️**

디코더의 임무
**잠재 공간에 있는 압축된 '핵심 요약본'을 입력받아, 원래의 데이터와 똑같은 모습으로 '복원'하는 것**
인코더가 했던 작업을 정확히 반대로 수행

- **인코더 (Encoder):** 고차원 원본 데이터 ➡️ 저차원 핵심 정보 (압축)
    
- **디코더 (Decoder):** 저차원 핵심 정보 ➡️ 고차원 복원 데이터 (복원)
    

다시 '글 요약' 비유
디코더는 요약본만 보고 원래의 장문으로 글을 다시 써내는 작가
요약본이 핵심을 잘 담고 있을수록, 복원된 글도 원본과 거의 비슷해짐.

이 전체 과정, 즉 **'원본'을 넣어서 '복원된 결과물'을 얻는 과정**에서 AI는 단 하나의 목표를 위해 학습함.
**'원본과 복원된 결과물의 차이(재구성 오류, Reconstruction Error)를 최소화하는 것'** 

이 오류를 줄이기 위해 애쓰는 과정에서, 인코더는 자연스럽게 데이터의 가장 중요한 특징을 추출하는 방법을, 디코더는 그 특징으로 원본을 재구성하는 방법을 터득하게 된다.


1. **활용 분야:** 그래서 이 기술을 실제로 어디에 쓰는지 알아보기 (ex: 이상치 탐지, 노이즈 제거)
    
2. **학습 원리:** '재구성 오류'를 줄인다는 게 구체적으로 어떤 의미인지 더 깊게 파고들기


